[train]
seed_val = 17
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
epochs = 12
batch_size = 8
seq_length = 128
lr = 1e-5
eps = 1e-8
# pretrained_model = bert-base-uncased
pretrained_model = bert_model/bert-base-uncased
test_size = 0.15
random_state = 2023
add_special_tokens = True
return_attention_mask = True
pad_to_max_length = True
do_lower_case = False
return_tensors = pt

output_size = 2

#768/2
hidden_dim = 384
n_layers = 2
drop_prob = 0.5
full_con1_out = 20
full_con2_out = 10


#这里为True，为双向LSTM
bidirectional = True

print_every = 20

# gradient clipping
clip=5

# log file name
loss_log = trainlog/loss
test_acc_log = trainlog/test_accuracy

modelPath = model/basic_classification_model

[data_split]
random_state = 2023
test_size = 0.3
valid_size = 0.5

[data]
train_path = sentimentData/marked sentences sentiment 2.xlsx

[classification]
sentence_vector_length = 128
batch_size = 256

[doc2vec]
epochs = 200
min_count = 5
window = 5
vector_size = 200
alpha = 0.025
min_alpha = 0.00025
dm = 1
workers = 8
negative = 5
sample = 1e-3
modelPath = model/doc2vec